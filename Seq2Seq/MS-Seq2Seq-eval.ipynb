{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b352c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36fe6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurations ---\n",
    "\n",
    "class CONFIG:\n",
    "    DATA_PATH = \"./kaggle/input/nfl-big-data-bowl-2026-prediction/\"\n",
    "\n",
    "    N_FOLDS = 5\n",
    "    EPOCHS = 10\n",
    "    PATIENCE = 5\n",
    "    FACTOR = .9\n",
    "    LEARNING_RATE = 5e-4\n",
    "    DROPOUT = 0.01\n",
    "    FORCE_MAX = .8\n",
    "    FORCE_MIN = .2\n",
    "\n",
    "\n",
    "config = CONFIG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222fc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Dataset -- \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, ids, x0, x1a, x1b, x2, duration):\n",
    "        # N: Number of training examples, L1: Length of input sequence, L2: Length of output sequence\n",
    "        self.ids = ids                  # str list      # shape: N\n",
    "        self.x0 = x0                    # np.arr(N)     # tensor(27)\n",
    "        self.x1a = x1a                  # np.arr(N)     # np.arr(L1, 10)\n",
    "        self.x1b = x1b                  # np.arr(N)     # np.arr(L1, 10)\n",
    "        self.x2 = x2                    # np.arr(N)     # tensor(2)\n",
    "        self.duration = duration        # int list      # shape: N\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.ids.iloc[idx]\n",
    "        x0 = self.x0[idx]\n",
    "        x1a = torch.tensor(self.x1a[idx], dtype=torch.float32)\n",
    "        if self.x1b[idx] is None: x1b = torch.empty(0, x1a.shape[1], dtype=torch.float32)\n",
    "        else: x1b = torch.tensor(self.x1b[idx], dtype=torch.float32)\n",
    "        x2 = self.x2[idx]\n",
    "        dur = self.duration[idx]\n",
    "        return (id, x0, x1a, x1b, x2, dur)  \n",
    "\n",
    "test_set = torch.load(\"test_set.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6dfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Model --\n",
    "\n",
    "class DotAttention(nn.Module):\n",
    "    def forward(self, s_t, memory):\n",
    "        s_t = s_t.unsqueeze(1)\n",
    "        scores = torch.bmm(s_t, memory.transpose(1, 2)).squeeze(1)\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), memory).squeeze(1)\n",
    "        return context\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=10, hid_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(in_dim, hid_dim, n_layers, batch_first=True, dropout=CONFIG.DROPOUT)\n",
    "        self.h0 = nn.Parameter(torch.randn(n_layers, 1, hid_dim))\n",
    "        self.c0 = nn.Parameter(torch.randn(n_layers, 1, hid_dim))\n",
    "\n",
    "    def forward(self, x1):\n",
    "        if x1.size(1) > 0:\n",
    "            e1, (h1, c1) = self.rnn(x1)\n",
    "        else:\n",
    "            e1 = torch.tensor([0])\n",
    "            h1 = self.h0\n",
    "            c1 = self.c0\n",
    "        return  e1, h1, c1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim=2, hid_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(out_dim, hid_dim, n_layers, batch_first=True, dropout=CONFIG.DROPOUT)\n",
    "        self.attn = DotAttention()\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(2*hid_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, y0, h_c, mem):\n",
    "        d0, h_c = self.rnn(y0, h_c)\n",
    "        d0 = torch.squeeze(d0, dim=1)\n",
    "        att = self.attn(d0, mem)\n",
    "        d0 = torch.concat([d0, att], dim=1)\n",
    "        d0 = self.out(d0)\n",
    "        return d0, h_c\n",
    "        \n",
    "\n",
    "class MultiScaleCNNLSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, in_dim=10, hid_dim=256, out_dim=2, const_dim=27):\n",
    "        super().__init__()\n",
    "        self.enc1 = Encoder()\n",
    "        self.enc2 = Encoder()\n",
    "        self.dec = Decoder()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hid_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hid_dim, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hid_dim, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.lin1 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin2 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin3 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin4 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin5 = nn.Linear(3*hid_dim, hid_dim)\n",
    "        self.lin6 = nn.Linear(3*hid_dim, hid_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x0, x1a, x1b, x2, dur, force=False, trg=None):\n",
    "        e1, h1, c1 = self.enc1(x1a)\n",
    "        _, h2, c2 = self.enc2(x1b)\n",
    "\n",
    "        x1a = torch.transpose(x1a, 1, 2)\n",
    "        e2 = torch.transpose(self.conv1(x1a), 1, 2)\n",
    "        e3 = torch.transpose(self.conv2(x1a), 1, 2)\n",
    "        e4 = torch.transpose(self.conv3(x1a), 1, 2)\n",
    "\n",
    "        mem = torch.concat([e1,e2,e3,e4], dim=1)\n",
    "        y0 = torch.unsqueeze(x2, 1)\n",
    "\n",
    "        h0 = torch.stack([self.lin1(x0), self.lin2(x0)], dim=0)\n",
    "        h0 = torch.concat([h0, h1, h2], dim=2)\n",
    "        h0 = F.relu(self.lin5(h0))      \n",
    "\n",
    "        c0 = torch.stack([self.lin3(x0), self.lin4(x0)], dim=0)\n",
    "        c0 = torch.concat([c0, c1, c2], dim=2)\n",
    "        c0 = F.relu(self.lin6(c0))\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for t in range(dur):\n",
    "            d0, (h0, c0) = self.dec(y0, (h0,c0), mem)\n",
    "            output.append(d0)\n",
    "            if force:\n",
    "                y0 = trg[:, t:(t+1), :]\n",
    "            else:\n",
    "                y0 = torch.unsqueeze(d0, dim=1)\n",
    "            \n",
    "        output = torch.stack(output).transpose(0,1)\n",
    "        return output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cf25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024120805_3701_56540_1</td>\n",
       "      <td>13.007598</td>\n",
       "      <td>37.380402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024120805_3701_56540_2</td>\n",
       "      <td>12.418032</td>\n",
       "      <td>37.615921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024120805_3701_56540_3</td>\n",
       "      <td>12.494217</td>\n",
       "      <td>38.165253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024120805_3701_56540_4</td>\n",
       "      <td>12.990677</td>\n",
       "      <td>38.053642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024120805_3701_56540_5</td>\n",
       "      <td>13.263040</td>\n",
       "      <td>38.237686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id          x          y\n",
       "0  2024120805_3701_56540_1  13.007598  37.380402\n",
       "1  2024120805_3701_56540_2  12.418032  37.615921\n",
       "2  2024120805_3701_56540_3  12.494217  38.165253\n",
       "3  2024120805_3701_56540_4  12.990677  38.053642\n",
       "4  2024120805_3701_56540_5  13.263040  38.237686"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Submission --\n",
    "\n",
    "model = MultiScaleCNNLSTMSeq2Seq()\n",
    "checkpoint = torch.load(\"MS-CNN-LSTM-Seq2Seq-1.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint)\n",
    "criterion = nn.MSELoss()\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "rows = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for id, x0, x1a, x1b, x2, dur in test_loader:\n",
    "        id = id[0]\n",
    "        preds = model(x0, x1a, x1b, x2, dur)[0]\n",
    "        \n",
    "        for frame_num, (x, y) in enumerate(preds, start=1):\n",
    "            rows.append({\n",
    "                \"id\": f\"{id}_{frame_num}\",\n",
    "                \"x\": x.item(),\n",
    "                \"y\": y.item()\n",
    "            })\n",
    "\n",
    "submission_df = pd.DataFrame(rows)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submitted!!!\")\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
