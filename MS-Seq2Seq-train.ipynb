{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurations ---\n",
    "\n",
    "class CONFIG:\n",
    "    DATA_PATH = \"./kaggle/input/nfl-big-data-bowl-2026-prediction/\"\n",
    "\n",
    "    N_FOLDS = 5\n",
    "    EPOCHS = 20\n",
    "    PATIENCE = 5\n",
    "    FACTOR = .9\n",
    "    LEARNING_RATE = 5e-4\n",
    "    DROPOUT = 0.01\n",
    "    FORCE_MAX = .8\n",
    "    FORCE_MIN = .2\n",
    "\n",
    "\n",
    "config = CONFIG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data import ---\n",
    "\n",
    "train_input_files = [f\"{config.DATA_PATH}train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "train_output_files = [f\"{config.DATA_PATH}train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "    \n",
    "train_input = pd.concat([pd.read_csv(f) for f in train_input_files])\n",
    "train_output = pd.concat([pd.read_csv(f) for f in train_output_files])   \n",
    "train_input_dict = {(g, p): subdf for (g, p), subdf in train_input.groupby(['game_id', 'play_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Feature engineering helpers --\n",
    "def height_to_feet(height_str):\n",
    "    try:\n",
    "        ft, inches = map(int, str(height_str).split('-'))\n",
    "        return ft + inches/12\n",
    "    except:\n",
    "        return 6.0\n",
    "\n",
    "def encode_play_dir(height_str):\n",
    "    return 1 if height_str==\"Left\" else 0\n",
    "\n",
    "def encode_player_role(height_str):\n",
    "    return 1 if height_str==\"Targeted Receiver\" else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Input constructors --\n",
    "\n",
    "def build_x0a(player_df):\n",
    "    height = height_to_feet(player_df.loc[0, 'player_height'])\n",
    "    weight = player_df.loc[0, 'player_weight']\n",
    "    land_x = player_df.loc[0, 'ball_land_x']\n",
    "    land_y = player_df.loc[0, 'ball_land_y']\n",
    "    start_x = player_df.loc[0, 'x']\n",
    "    start_y = player_df.loc[0, 'y']\n",
    "    end_x = player_df.iloc[-1]['x']\n",
    "    end_y = player_df.iloc[-1]['y']\n",
    "    end_s = player_df.iloc[-1]['s']\n",
    "    end_a = player_df.iloc[-1]['a']\n",
    "    end_dir = math.radians(player_df.iloc[-1]['dir'])\n",
    "    end_o = math.radians(player_df.iloc[-1]['o'])\n",
    "    end_vx = end_s * math.sin(end_dir)\n",
    "    end_vy = end_s * math.cos(end_dir)\n",
    "    end_ox = math.sin(end_o)\n",
    "    end_oy = math.cos(end_o)\n",
    "    play_dir = encode_play_dir(player_df.loc[0, 'play_direction'])\n",
    "    role = encode_player_role(player_df.loc[0, 'player_role'])\n",
    "    duration = player_df.loc[0, 'num_frames_output']\n",
    "    return (np.array([height, weight, land_x, land_y, start_x, start_y, end_x, end_y, end_a, end_vx, end_vy, end_ox, end_oy, play_dir, role, duration]), duration)\n",
    "\n",
    "def build_x0b(qb_df):\n",
    "    if qb_df.empty:\n",
    "        return np.array([6.25, 220, 60, 26.6, 60, 26.6, 0, 0, 0, 0, 0])\n",
    "    height = height_to_feet(qb_df.loc[0, 'player_height'])\n",
    "    weight = qb_df.loc[0, 'player_weight']\n",
    "    start_x = qb_df.loc[0, 'x']\n",
    "    start_y = qb_df.loc[0, 'y']\n",
    "    end_x = qb_df.iloc[-1]['x']\n",
    "    end_y = qb_df.iloc[-1]['y']\n",
    "    end_s = qb_df.iloc[-1]['s']\n",
    "    end_a = qb_df.iloc[-1]['a']\n",
    "    end_dir = math.radians(qb_df.iloc[-1]['dir'])\n",
    "    end_o = math.radians(qb_df.iloc[-1]['o'])\n",
    "    end_vx = end_s * math.sin(end_dir)\n",
    "    end_vy = end_s * math.cos(end_dir)\n",
    "    end_ox = math.sin(end_o)\n",
    "    end_oy = math.cos(end_o)\n",
    "    return np.array([height, weight, start_x, start_y, end_x, end_y, end_a, end_vx, end_vy, end_ox, end_oy])\n",
    "\n",
    "\n",
    "def build_x1(player_df):\n",
    "    dir_radians = np.deg2rad(player_df['dir'])\n",
    "    o_radians = np.deg2rad(player_df['o'])\n",
    "    player_df['vx'] = player_df['s'] * np.sin(dir_radians)\n",
    "    player_df['vy'] = player_df['s'] * np.cos(dir_radians)\n",
    "    player_df['ax'] = player_df['vx'].diff().fillna(0)\n",
    "    player_df['ay'] = player_df['vy'].diff().fillna(0)\n",
    "    player_df['ox'] = np.sin(o_radians)\n",
    "    player_df['oy'] = np.cos(o_radians)\n",
    "    return player_df[['x', 'y', 'vx', 'vy', 'ax', 'ay', 's', 'a', 'ox', 'oy']].to_numpy()\n",
    "\n",
    "def build_x2(player_df):\n",
    "    end_x = player_df.iloc[-1]['x']\n",
    "    end_y = player_df.iloc[-1]['y']\n",
    "    return np.array([end_x, end_y])\n",
    "\n",
    "\n",
    "def build_inputs(gid, pid, nid):\n",
    "    in_df = train_input_dict[(gid, pid)]\n",
    "    player_df = in_df[in_df['nfl_id'] == nid].copy().reset_index(drop=True)\n",
    "    qb_df = in_df[in_df['player_role'] == 'Passer'].copy().reset_index(drop=True)\n",
    "\n",
    "    x0a, dur = build_x0a(player_df)\n",
    "    x0b = build_x0b(qb_df)\n",
    "    x0 = np.concatenate((x0a, x0b), axis=0)\n",
    "    x0 = torch.from_numpy(x0).float()\n",
    "\n",
    "    x1a = build_x1(player_df)\n",
    "    x1b = build_x1(qb_df)\n",
    "    if x1b.size==0: x1b = None\n",
    "\n",
    "    x2 = build_x2(player_df)\n",
    "    x2 = torch.from_numpy(x2).float()\n",
    "\n",
    "    return (x0, x1a, x1b, x2, dur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Dataset and Dataloader-- \n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, ids, x0, x1a, x1b, x2, duration, targets):\n",
    "        # N: Number of training examples, L1: Length of input sequence, L2: Length of output sequence\n",
    "        self.ids = ids                  # str list      # shape: N\n",
    "        self.x0 = x0                    # np.arr(N)     # tensor(27)\n",
    "        self.x1a = x1a                  # np.arr(N)     # np.arr(L1, 10)\n",
    "        self.x1b = x1b                  # np.arr(N)     # np.arr(L1, 10)\n",
    "        self.x2 = x2                    # np.arr(N)     # tensor(2)\n",
    "        self.targets = targets          # np.arr(N)     # np.arr(L2, 2)\n",
    "        self.duration = duration        # int list      # shape: N\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.ids[idx]\n",
    "        x0 = self.x0[idx]\n",
    "        x1a = torch.tensor(self.x1a[idx], dtype=torch.float32)\n",
    "        if self.x1b[idx] is None: x1b = torch.empty(0, x1a.shape[1], dtype=torch.float32)\n",
    "        else: x1b = torch.tensor(self.x1b[idx], dtype=torch.float32)\n",
    "        x2 = self.x2[idx]\n",
    "        dur = self.duration[idx]\n",
    "        targets = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return (id, x0, x1a, x1b, x2, targets, dur)  \n",
    "\n",
    "def build_train_ds(train_output):\n",
    "    train = train_output.groupby(['game_id', 'play_id', 'nfl_id'])[['x', 'y']].apply(lambda a: a.to_numpy()).reset_index(name = 'targets')\n",
    "    train[['x0', 'x1a', 'x1b', 'x2', 'dur']] = train.apply(\n",
    "        lambda row: pd.Series(build_inputs(row['game_id'], row['play_id'], row['nfl_id'])),\n",
    "        axis=1\n",
    "    )\n",
    "    train[\"id\"] = train[\"game_id\"].astype(str) + \"_\" + train[\"play_id\"].astype(str) + \"_\" + train[\"nfl_id\"].astype(str)\n",
    "    train = TrainDataset(\n",
    "        train['id'], \n",
    "        train['x0'].to_numpy(), \n",
    "        train['x1a'].to_numpy(), \n",
    "        train['x1b'].to_numpy(), \n",
    "        train['x2'].to_numpy(), \n",
    "        train['dur'].to_numpy(), \n",
    "        targets = train['targets'].to_numpy(),\n",
    "    )\n",
    "    return train\n",
    "\n",
    "train_set = build_train_ds(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Model --\n",
    "\n",
    "class DotAttention(nn.Module):\n",
    "    def forward(self, s_t, memory):\n",
    "        s_t = s_t.unsqueeze(1)\n",
    "        scores = torch.bmm(s_t, memory.transpose(1, 2)).squeeze(1)\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), memory).squeeze(1)\n",
    "        return context\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=10, hid_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(in_dim, hid_dim, n_layers, batch_first=True, dropout=CONFIG.DROPOUT)\n",
    "        self.h0 = nn.Parameter(torch.randn(n_layers, 1, hid_dim))\n",
    "        self.c0 = nn.Parameter(torch.randn(n_layers, 1, hid_dim))\n",
    "\n",
    "    def forward(self, x1):\n",
    "        if x1.size(1) > 0:\n",
    "            e1, (h1, c1) = self.rnn(x1)\n",
    "        else:\n",
    "            e1 = torch.tensor([0])\n",
    "            h1 = self.h0\n",
    "            c1 = self.c0\n",
    "        return  e1, h1, c1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim=2, hid_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(out_dim, hid_dim, n_layers, batch_first=True, dropout=CONFIG.DROPOUT)\n",
    "        self.attn = DotAttention()\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(2*hid_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, y0, h_c, mem):\n",
    "        d0, h_c = self.rnn(y0, h_c)\n",
    "        d0 = torch.squeeze(d0, dim=1)\n",
    "        att = self.attn(d0, mem)\n",
    "        d0 = torch.concat([d0, att], dim=1)\n",
    "        d0 = self.out(d0)\n",
    "        return d0, h_c\n",
    "        \n",
    "\n",
    "class MultiScaleCNNLSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, in_dim=10, hid_dim=256, out_dim=2, const_dim=27):\n",
    "        super().__init__()\n",
    "        self.enc1 = Encoder()\n",
    "        self.enc2 = Encoder()\n",
    "        self.dec = Decoder()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hid_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hid_dim, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hid_dim, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hid_dim, hid_dim, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.lin1 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin2 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin3 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin4 = nn.Linear(const_dim, hid_dim)\n",
    "        self.lin5 = nn.Linear(3*hid_dim, hid_dim)\n",
    "        self.lin6 = nn.Linear(3*hid_dim, hid_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x0, x1a, x1b, x2, dur, force=False, trg=None):\n",
    "        e1, h1, c1 = self.enc1(x1a)\n",
    "        _, h2, c2 = self.enc2(x1b)\n",
    "\n",
    "        x1a = torch.transpose(x1a, 1, 2)\n",
    "        e2 = torch.transpose(self.conv1(x1a), 1, 2)\n",
    "        e3 = torch.transpose(self.conv2(x1a), 1, 2)\n",
    "        e4 = torch.transpose(self.conv3(x1a), 1, 2)\n",
    "\n",
    "        mem = torch.concat([e1,e2,e3,e4], dim=1)\n",
    "        y0 = torch.unsqueeze(x2, 1)\n",
    "\n",
    "        h0 = torch.stack([self.lin1(x0), self.lin2(x0)], dim=0)\n",
    "        h0 = torch.concat([h0, h1, h2], dim=2)\n",
    "        h0 = F.relu(self.lin5(h0))      \n",
    "\n",
    "        c0 = torch.stack([self.lin3(x0), self.lin4(x0)], dim=0)\n",
    "        c0 = torch.concat([c0, c1, c2], dim=2)\n",
    "        c0 = F.relu(self.lin6(c0))\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for t in range(dur):\n",
    "            d0, (h0, c0) = self.dec(y0, (h0,c0), mem)\n",
    "            output.append(d0)\n",
    "            if force:\n",
    "                y0 = trg[:, t:(t+1), :]\n",
    "            else:\n",
    "                y0 = torch.unsqueeze(d0, dim=1)\n",
    "            \n",
    "        output = torch.stack(output).transpose(0,1)\n",
    "        return output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(x0, x1a, x1b, x2, dur, force\u001b[38;5;241m=\u001b[39mforce, trg\u001b[38;5;241m=\u001b[39mtarget)\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n\u001b[0;32m---> 40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m rmse \u001b[38;5;241m=\u001b[39m (loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5 Fold Cross Validation Training\n",
    "kf = KFold(n_splits=CONFIG.N_FOLDS, shuffle=True, random_state=1)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_set)):\n",
    "    print(f\"Fold #{fold}\")\n",
    "\n",
    "    # Data prep\n",
    "    train_subset = Subset(train_set, train_idx)\n",
    "    val_subset   = Subset(train_set, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=1, shuffle=True)\n",
    "    val_loader   = DataLoader(val_subset, batch_size=1)\n",
    "\n",
    "    # Model prep\n",
    "    model = MultiScaleCNNLSTMSeq2Seq()\n",
    "    #checkpoint = torch.load(\"MS-CNN-LSTM-Seq2Seq-1.pth\", weights_only=True)\n",
    "    #model.load_state_dict(checkpoint)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG.LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode = 'min',\n",
    "        factor = CONFIG.FACTOR,\n",
    "        patience = CONFIG.PATIENCE,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    train_rmse = 0.0\n",
    "    for epoch in range(1, CONFIG.EPOCHS+1):\n",
    "\n",
    "        force_ratio = CONFIG.FORCE_MAX - ((epoch-1)/CONFIG.EPOCHS)*(CONFIG.FORCE_MAX - CONFIG.FORCE_MIN)\n",
    "        pbar = tqdm(train_loader, leave=False)\n",
    "\n",
    "        for index, (id, x0, x1a, x1b, x2, target, dur) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            force = torch.rand(1).item() < force_ratio\n",
    "\n",
    "            pred = model(x0, x1a, x1b, x2, dur, force=force, trg=target)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            rmse = (loss.item()) ** 0.5\n",
    "            train_rmse += rmse\n",
    "            if index%200==0:pbar.set_postfix({'Loss': f\"{rmse:.4f}\"})\n",
    "\n",
    "        train_rmse = train_rmse/len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_rmse = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for id, x0, x1a, x1b, x2, target, dur in val_loader:\n",
    "                pred = model(x0, x1a, x1b, x2, dur, force=False)\n",
    "                loss = criterion(pred, target)\n",
    "                val_rmse += (loss.item()) ** 0.5\n",
    "            val_rmse = val_rmse/len(val_loader)\n",
    "\n",
    "        # Epoch over\n",
    "        print(f\"Epoch {epoch} Train loss: {(train_rmse):.4f} Val loss: {(val_rmse):.4f}\")\n",
    "        scheduler.step(val_rmse)\n",
    "        torch.save(model.state_dict(), \"MS-CNN-LSTM-Seq2Seq-1.pth\")\n",
    "        \n",
    "    break\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
